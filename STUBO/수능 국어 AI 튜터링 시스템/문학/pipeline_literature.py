# -*- coding: utf-8 -*-
"""pipeline_literature.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1elspAdOPFyuf2DwlM2L92IEkMVi2RI6y

# **0. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ & api key**
"""

import os

if __name__ == "__main__":  # ì´ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ
    try:
        from google.colab import drive
        drive.mount('/content/drive')
    except:
        pass

import os
import openai
from langchain.chat_models import ChatOpenAI

#  API í‚¤ ì„¤ì •
OPENAI_API_KEY = "  "
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
client = openai.OpenAI(api_key=OPENAI_API_KEY)
openai.api_key = OPENAI_API_KEY

from langchain.document_loaders import PyPDFLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from PIL import Image
import os
import json
import numpy as np

"""# **<Retriever (ë‹µë³€,í•´ì„¤ / ìœ ì‚¬ë¬¸ì œ ì¶”ì²œ)> í˜¸ì¶œ**"""

# ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ
VECTORSTORE_PATH = "/content/drive/MyDrive/Colab Notebooks/TAVE í”„ë¡œì íŠ¸_STUBO/ìˆ˜ëŠ¥ êµ­ì–´ AI íŠœí„°ë§ ì‹œìŠ¤í…œ/ë¬¸í•™/faiss_index_ë‹µë³€í•´ì„¤"

# retriever ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def get_retriever():
    if not os.path.exists(VECTORSTORE_PATH):
        raise FileNotFoundError(f"âŒ ë²¡í„°ìŠ¤í† ì–´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {VECTORSTORE_PATH}")

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = FAISS.load_local(VECTORSTORE_PATH, embeddings, allow_dangerous_deserialization=True)

    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 5,
            "score_threshold": 0.5
        }
    )
    return retriever

# retriever ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
retriever_literature_answer = get_retriever()

# ğŸ”¹ ê²½ë¡œ ì„¤ì •
index_path = "/content/drive/MyDrive/Colab Notebooks/TAVE í”„ë¡œì íŠ¸_STUBO/ìˆ˜ëŠ¥ êµ­ì–´ AI íŠœí„°ë§ ì‹œìŠ¤í…œ/ë¬¸í•™/faiss_index_ìœ ì‚¬ë¬¸ì œ"
embedding_model = OpenAIEmbeddings()

# ğŸ”¹ ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ
retriever_literature_recommend = FAISS.load_local(index_path, embedding_model, allow_dangerous_deserialization=True)

"""# **<ì´ë¯¸ì§€(ì§€ë¬¸/ë¬¸í•­)ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ>**
- ì§€ë¬¸ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œí• ë•Œ ë¹„ìš© ë¬¸ì œ ë°œìƒ

### **ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ**
ì´ë¯¸ì§€ 5ë“±ë¶„ -> gpt-4oë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ -> gpt-4oê°€ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ë¹„êµí•˜ë©´ íŠ¹ìˆ˜ë¬¸ì ì‚½ì… -> gpt-4oê°€ íŠ¹ìˆ˜ë¬¸ì ê²€í†  -> gpt-4oê°€ ì§€ë¬¸ ë²”ìœ„([A], [B] ë“±) ì‚½ì…
"""

from PIL import Image
import openai
import base64
import io
from typing import List, Tuple

# âœ… ì´ë¯¸ì§€ â†’ base64
def image_to_base64(image: Image.Image) -> str:
    buffer = io.BytesIO()
    image.save(buffer, format="PNG")
    return f"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}"

# âœ… ì´ë¯¸ì§€ ìˆ˜ì§ ë¶„í• 
def split_image_vertically(image: Image.Image, parts: int = 5) -> List[Image.Image]:
    width, height = image.size
    part_height = height // parts
    return [
        image.crop((0, i * part_height, width, height if i == parts - 1 else (i + 1) * part_height))
        for i in range(parts)
    ]

# âœ… GPTì—ê²Œ OCR ì‹œí‚¤ëŠ” í•¨ìˆ˜
def gpt_ocr_text(image: Image.Image) -> str:
    base64_img = image_to_base64(image)

    system_prompt = """
    ë„ˆëŠ” ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ì˜ ì§€ë¬¸ OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì „ë¬¸ê°€ì•¼.

    ì•„ë˜ ì´ë¯¸ì§€ë¥¼ ë³´ê³  **OCR í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì¶”ì¶œ**í•´.
    í…ìŠ¤íŠ¸ ì¶”ì¶œë§Œ í•˜ê³ , ì ˆëŒ€ ê°€ê³µí•˜ê±°ë‚˜ ì„¤ëª…í•˜ì§€ ë§ˆ.

    ğŸ“Œ ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ:
    - ì¤„ë°”ê¿ˆì€ ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ê·¸ëŒ€ë¡œ ì‚´ë ¤ì•¼ í•´.
    - ë„ì–´ì“°ê¸°, íŠ¹ìˆ˜ë¬¸ì, ê´„í˜¸, ë§ˆì¹¨í‘œ ë“± ëª¨ë“  ë¬¸ì¥ ë¶€í˜¸ë„ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì•¼ í•´.
    - í•´ì„ì´ë‚˜ ë¶€ì—° ì„¤ëª… ì—†ì´ **ìˆœìˆ˜í•œ OCR ê²°ê³¼ë§Œ ì¶œë ¥**í•´ì•¼ í•´.
    """

    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt.strip()},
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {
                            "url": base64_img,
                            "detail": "high"
                        }}
                    ]
                }
            ],
            temperature=0,
            max_tokens=16000
        )
        output = response.choices[0].message.content.strip()
        if not output or "ì£„ì†¡í•˜ì§€ë§Œ" in output:
            raise ValueError("GPT OCR ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ")
        return output
    except Exception as e:
        return f"[âŒ GPT OCR ì‹¤íŒ¨]: {str(e)}"

# âœ… GPT-4oë¡œ íŠ¹ìˆ˜ê¸°í˜¸&ê´„í˜¸ ì‚½ì…
def refine_text_with_gpt(image: Image.Image, ocr_output: str) -> str:
    base64_img = image_to_base64(image)

    system_prompt = """
    ë„ˆëŠ” ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ ì§€ë¬¸ ì •ë¦¬ ì „ë¬¸ê°€ì•¼.

    ë‹¤ìŒì€ OCRë¡œ ì¶”ì¶œí•œ ì§€ë¬¸ê³¼ ì›ë³¸ ì´ë¯¸ì§€ì•¼. ë„ˆëŠ” ì´ ë‘ ì •ë³´ë¥¼ ë¹„êµí•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ OCR ê²°ê³¼ë¥¼ ì •í™•í•˜ê²Œ ìˆ˜ì •í•´ì•¼ í•´.

    ğŸ“Œ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ê·œì¹™:

    1. âœ… ì´ë¯¸ì§€ ì•ˆì—ì„œ íŠ¹ìˆ˜ê¸°í˜¸(ì›í˜• ë¬¸ì/ì•ŒíŒŒë²³)ë¥¼ ì •í™•íˆ ì‹ë³„í•˜ê³ , OCR í…ìŠ¤íŠ¸ì˜ ì•Œë§ì€ ìœ„ì¹˜ì— ì‚½ì…í•´ì•¼ í•´.
      - OCR ê²°ê³¼ë§Œ ë³´ë©´ ì•ˆ ë¼. ë°˜ë“œì‹œ ì´ë¯¸ì§€ì—ì„œ íŠ¹ìˆ˜ê¸°í˜¸ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•´ì•¼ í•´.

    2. âœ… ì‚½ì…í•´ì•¼ í•  íŠ¹ìˆ˜ê¸°í˜¸ëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
      - í•œê¸€ ì›í˜• ë¬¸ì: ã‰ , ã‰¡, ã‰¢, ã‰£, ã‰¤
      - ì›í˜• ì•ŒíŒŒë²³: â“, â“‘, â“’, â““, â“”

    3. âœ… **ëª¨ë“  íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì—ëŠ” ë°‘ì¤„ë¡œ ê°•ì¡°ëœ ë¬¸ì¥ì´ ë°˜ë“œì‹œ ìˆê³ **, ê·¸ **ê°•ì¡°ëœ ë¬¸ì¥ ì „ì²´ë¥¼ ê´„í˜¸ '( )'ë¡œ ë°˜ë“œì‹œ ê°ì‹¸ì•¼ í•´.**
      - íŠ¹ìˆ˜ê¸°í˜¸ì™€ ê´„í˜¸ëŠ” ë¶™ì—¬ì„œ ì‘ì„±í•´: ì˜ˆ) â“(ê°•ì¡°ëœ ë¬¸ì¥)
      - ê´„í˜¸ëŠ” í•´ë‹¹ ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì„ ì •í™•íˆ ê°ì‹¸ì•¼ í•˜ë©°, ì¤„ë°”ê¿ˆì´ë‚˜ ê³µë°±ì´ ìˆì–´ë„ ì „ì²´ë¥¼ í¬í•¨í•´ì•¼ í•´.

    4. âœ… í•œ íŠ¹ìˆ˜ê¸°í˜¸ì— í•´ë‹¹í•˜ëŠ” ê°•ì¡° ë¬¸ì¥ì´ **ì—¬ëŸ¬ ì¤„ì— ê±¸ì³ ìˆë”ë¼ë„** ê´„í˜¸ë¡œ ì •í™•íˆ ê°ì‹¸ì•¼ í•´.
      - ì¤‘ê°„ ì¤„ë°”ê¿ˆì´ë‚˜ ê³µë°±ì´ ìˆë”ë¼ë„ ê°•ì¡° ë¬¸ì¥ ì „ì²´ê°€ ê´„í˜¸ ì•ˆì— ë“¤ì–´ê°€ì•¼ í•´.

    5. âŒ íŠ¹ìˆ˜ê¸°í˜¸ê°€ ì—†ëŠ” ë¬¸ì¥ì€ ìˆ˜ì •í•˜ì§€ ë§ˆ.
      âŒ ì² ì ì˜¤ë¥˜, ë„ì–´ì“°ê¸° ì˜¤ë¥˜ ë“± OCR ìì²´ ì˜¤ë¥˜ë„ ê³ ì¹˜ì§€ ë§ˆ.

    6. âœ… ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§€ë¬¸ ì „ì²´ë¥¼ í¬í•¨í•´ì•¼ í•˜ë©°, íŠ¹ìˆ˜ê¸°í˜¸ + ê°•ì¡°ë¬¸ì¥ ë¶€ë¶„ë§Œ ìˆ˜ì •í•´ì•¼ í•´.
      âŒ ì„¤ëª…, í•´ì„¤, ì¶”ê°€ ì •ë³´ ì—†ì´ **ì§€ë¬¸ ì „ì²´ë§Œ ì¶œë ¥**í•´ì•¼ í•´.


     âš ï¸ ë°˜ë“œì‹œ íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì—ëŠ” ë°‘ì¤„ë¡œ ê°•ì¡°ëœ ë¶€ë¶„ì´ ê´„í˜¸'()'ë¡œ ê°ì‹¸ì ¸ ìˆì–´ì•¼í•´.
     âš ï¸ íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì— ê´„í˜¸ê°€ ì—†ë‹¤ë©´ ë‹¤ì‹œ ë°‘ì¤„ë¡œ ê°•ì¡°ëœ ë¶€ë¶„ì„ ë‹¤ì‹œ ì°¾ê³  ê´„í˜¸ë¥¼ ì¶”ê°€í•´ì¤˜.
    """

    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt.strip()},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": f"OCR ì¶”ì¶œ ê²°ê³¼:\n{ocr_output.strip()}"},
                        {"type": "image_url", "image_url": {
                            "url": base64_img,
                            "detail": "auto"
                        }}
                    ]
                }
            ],
            temperature=0.1,
            max_tokens=16000  # ê°€ëŠ¥í•œ ìµœëŒ€ê°’ ì‚¬ìš©
        )
        output = response.choices[0].message.content.strip()
        if not output or "ì£„ì†¡í•˜ì§€ë§Œ" in output or "ë„ì™€ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in output:
            raise ValueError("GPT ì‘ë‹µ ì˜¤ë¥˜ ë˜ëŠ” ë‚´ìš© ì—†ìŒ")
        return output
    except Exception as e:
        return f"[âŒ GPT ì •êµí™” ì‹¤íŒ¨]: {str(e)}"

# âœ… íŠ¹ìˆ˜ê¸°í˜¸ ìœ„ì¹˜ ì ê²€ ë° ë³´ì • ìš”ì²­
def verify_special_symbols(original_image: Image.Image, restored_text: str) -> str:
    base64_img = image_to_base64(original_image)

    system_prompt = """
      ë„ˆëŠ” ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ì˜ ì§€ë¬¸ OCR ë³µì› ì „ë¬¸ê°€ì•¼.

      ë‹¤ìŒì€ OCRë¡œ ì¶”ì¶œí•œ ì§€ë¬¸ê³¼ ì›ë³¸ ì´ë¯¸ì§€ì•¼. ë„ˆëŠ” ì´ ë‘ ì •ë³´ë¥¼ ë¹„êµí•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ OCR ê²°ê³¼ë¥¼ ì •í™•í•˜ê²Œ ìˆ˜ì •í•´ì•¼ í•´.
      ì¶”ì¶œí•œ ì§€ë¬¸ì˜ íŠ¹ìˆ˜ê¸°í˜¸(ã‰ , ã‰¡, ã‰¢, ã‰£, ã‰¤, â“, â“‘, â“’, â““, â“” ë“±)ê°€ ì›ë³¸ ì´ë¯¸ì§€ì˜ íŠ¹ìˆ˜ê¸°í˜¸ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ì„œ ìˆ˜ì •í•´ì¤˜.

      ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë³´ê³ 
      - ë¬¸ì¥ ì• ê¸°í˜¸ê°€ ì´ë¯¸ì§€ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ ,
      - ì˜ëª»ëœ ê¸°í˜¸ëŠ” ì˜¬ë°”ë¥¸ íŠ¹ìˆ˜ê¸°í˜¸ë¡œ ìˆ˜ì •í•´.
      - ì´ë¯¸ì§€ì—ëŠ” íŠ¹ìˆ˜ê¸°í˜¸ê°€ ì—†ëŠ”ë° í…ìŠ¤íŠ¸ì—ëŠ” ìˆëŠ” ê²½ìš°ëŠ” íŠ¹ìˆ˜ê¸°í˜¸ë¥¼ ì‚­ì œí•´ì¤˜.
      - **ì¶”ì¶œí•œ ì§€ë¬¸ì˜ íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì— ê´„í˜¸'('ê°€ ì—†ë‹¤ë©´ ì´ë¯¸ì§€ì—ì„œ ë°‘ì¤„ë¡œ ê°•ì¡°ëœ ë¶€ë¶„ì„ ì°¾ì•„ì„œ ê´„í˜¸ë¡œ ê°ì‹¸ì„œ ì§€ë¬¸ì„ ìˆ˜ì •í•´ì¤˜.**

      âš ï¸ â‘ , â‘¡, â‘¢, â‘£, â‘¤ ì´ëŸ° ì›í˜• ìˆ«ì íŠ¹ìˆ˜ê¸°í˜¸ëŠ” ì§€ë¬¸ì— ìˆì„ ìˆ˜ ì—†ì–´ ë°˜ë“œì‹œ ë‹¤ì‹œ ì˜¬ë°”ë¥¸ íŠ¹ìˆ˜ê¸°í˜¸ë¡œ ìˆ˜ì •í•´ì¤˜.
      âš ï¸ ë™ì¼í•œ íŠ¹ìˆ˜ê¸°í˜¸ê°€ ë˜ ë‚˜ì˜¬ ìˆ˜ ì—†ì–´ ë‹¤ì‹œ ì˜¬ë°”ë¥¸ íŠ¹ìˆ˜ê¸°í˜¸ë¡œ ìˆ˜ì •í•´ì¤˜.
      âš ï¸ ë°˜ë“œì‹œ íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì—ëŠ” ë°‘ì¤„ë¡œ ê°•ì¡°ëœ ë¶€ë¶„ì´ ê´„í˜¸'()'ë¡œ ë°˜ë“œì‹œ ê°ì‹¸ì ¸ ìˆì–´ì•¼í•´.
      âš ï¸ ì¶”ì¶œí•œ ì§€ë¬¸ì˜ íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì— ê´„í˜¸ê°€ ì—†ë‹¤ë©´ ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì— ë‹¤ì‹œ ë°‘ì¤„ë¡œ ê°•ì¡°ëœ ë¶€ë¶„ì„ ë‹¤ì‹œ ì°¾ê³  ì§€ë¬¸ í…ìŠ¤íŠ¸ì— ê´„í˜¸'()'ë¥¼ ì¶”ê°€í•´ì¤˜.
      âš ï¸ ì´ë¯¸ì§€ ê¸°í˜¸ ìœ„ì¹˜ë¥¼ ê¼­ í™•ì¸í•´ íŒë‹¨í•˜ê³ ,
      âš ï¸ ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§€ë¬¸ ì „ì²´ë¥¼ ì¶œë ¥í•´.
    """

    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt.strip()},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": f"ë³µì›ëœ ì§€ë¬¸ ì¼ë¶€:\n{restored_text.strip()}"},
                        {"type": "image_url", "image_url": {
                            "url": base64_img,
                            "detail": "high"
                        }}
                    ]
                }
            ],
            temperature=0.1,
            max_tokens=16000
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[âŒ GPT ê¸°í˜¸ ê²€í†  ì‹¤íŒ¨]: {str(e)}"

# ì´ë¯¸ì§€ ìš°ì¸¡ ì§€ë¬¸ ë²”ìœ„ í‘œì‹œ
def insert_passage_brackets_with_gpt(image: Image.Image, ocr_text: str) -> str:
    base64_img = image_to_base64(image)

    system_prompt = """
    ë„ˆëŠ” ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ì˜ ì§€ë¬¸ OCR ë³µì› ì „ë¬¸ê°€ì•¼.

    ë‹¤ìŒì€ OCRë¡œ ì¶”ì¶œí•œ ì§€ë¬¸ê³¼ ì›ë³¸ ì´ë¯¸ì§€ì•¼.
    ì›ë³¸ ì´ë¯¸ì§€ì˜ **ì¢Œì¸¡ í˜¹ì€ ìš°ì¸¡ì— [A], [B], [C] ë“±ì˜ í‘œì‹œì™€ í•¨ê»˜ íŠ¹ì • ì§€ë¬¸ ë²”ìœ„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„ ë“¤ì´ ìˆëŠ” ê²½ìš°ê°€ ìˆì–´.**
    ì´ ì‹œê°ì  ì •ë³´ëŠ” ì–´ë–¤ ì§€ë¬¸ êµ¬ê°„ì´ ë¬¸ì œ í’€ì´ì—ì„œ ì¤‘ìš”í•œì§€ë¥¼ ì•Œë ¤ì£¼ëŠ” ë‹¨ì„œì•¼.

    ğŸ” ë„ˆì˜ ì„ë¬´ëŠ” ë‹¤ìŒê³¼ ê°™ì•„:

    1. ì§€ë¬¸ ì´ë¯¸ì§€ì˜ ë§¨ ì™¼ìª½ê³¼ ë§¨ ì˜¤ë¥¸ìª½ì„ ì˜ ì‚´í´ë³´ê³ ,
      íŠ¹ì • ì§€ë¬¸ ë²”ìœ„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„ ê³¼ í•¨ê»˜ [A], [B], [C] ë“±ì´ ì§€ë¬¸ ë‚´ **ì–´ë””ì„œë¶€í„° ì–´ë””ê¹Œì§€ë¥¼ ê°€ë¦¬í‚¤ëŠ”ì§€** íŒë‹¨í•´.
        - ê° ë²”ìœ„ëŠ” ë°˜ë“œì‹œ **1ì¤„ ì´ìƒ**ì´ ë˜ë„ë¡ í•˜ê³ , **ë¬¸ì¥ì´ ëŠê¸°ì§€ ì•Šê²Œ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨**í•´ì•¼ í•´.
        - ë¬¸ì¥ì˜ ì‹œì‘ì´ë‚˜ ëì´ ì˜ë¦¬ì§€ ì•Šë„ë¡, ì§€ë¬¸ íë¦„ì— ë§ê²Œ í•´ë‹¹ **ì‹œì‘ ì¤„ê³¼ ë ì¤„ ì „ì²´ë¥¼ í¬í•¨**í•´ì•¼ í•´.

    2. ë§Œì•½ íŠ¹ì • ì§€ë¬¸ ë²”ìœ„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„ ì´ ìˆë‹¤ë©´, íŒë‹¨ëœ ë²”ìœ„ë¥¼ ì§€ë¬¸ ì¤‘ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ ì•„ë˜ ì˜ˆì‹œì²˜ëŸ¼ `[A]{}`ë¡œ ê°ì‹¸ ê°•ì¡°í•´ì¤˜:
        ...
        [A] {
            (í•´ë‹¹ ë²”ìœ„ ì‹œì‘ ì¤„)
            ...
            (í•´ë‹¹ ë²”ìœ„ ë ì¤„)
        }
        ...

    âš ï¸ ìœ ì˜ì‚¬í•­:
    - ì¤„ ë‹¨ìœ„ë¡œ íŒë‹¨í•˜ë˜, ì˜ë¯¸ ë‹¨ìœ„(ë¬¸ì¥ êµ¬ì¡°)ë¥¼ ìµœëŒ€í•œ ìœ ì§€í•´ì•¼ í•´.
    - íŠ¹ì • ì§€ë¬¸ ë²”ìœ„ì˜ ì‹œì‘ê³¼ ë ìœ„ì¹˜ëŠ” ì´ë¯¸ì§€ ì˜¤ë¥¸ìª½ì˜ ì„ ê³¼ ì‹œê°ì ìœ¼ë¡œ ì •ë ¬ëœ ì§€ë¬¸ ì¤„ì„ ì°¾ì•„ íŒë‹¨í•´ì•¼ í•´.
    - ì¤„ë°”ê¿ˆ, ê³µë°±, ê´„í˜¸, íŠ¹ìˆ˜ê¸°í˜¸ ë“±ì€ OCR í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•´.
    """

    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt.strip()},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": f"OCR ê²°ê³¼:\n{ocr_text.strip()}"},
                        {"type": "image_url", "image_url": {
                            "url": base64_img,
                            "detail": "high"
                        }}
                    ]
                }
            ],
            temperature=0.1,
            max_tokens=16000
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[âŒ GPT ì§€ë¬¸ ë²”ìœ„ ì‚½ì… ì‹¤íŒ¨]: {str(e)}"

import os
from PIL import Image

def run_split_pipeline(image_path: str, parts: int = 4):
    print("ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...")
    image = Image.open(image_path)

    # 1. ì´ë¯¸ì§€ ìˆ˜ì§ ë¶„í• 
    print(f"ğŸ”€ ì´ë¯¸ì§€ {parts}ë“±ë¶„ ì¤‘...")
    split_images = split_image_vertically(image, parts=parts)

    # 2. ê° ë¶„í•  ì´ë¯¸ì§€ OCR
    ocr_parts = []
    for idx, part_img in enumerate(split_images):
        print(f"ğŸ” [{idx + 1}/{parts}] ë¶„í•  ì´ë¯¸ì§€ OCR ì¤‘...")
        part_text = gpt_ocr_text(part_img)
        ocr_parts.append(part_text)

    # 3. OCR ê²°ê³¼ ë³‘í•©
    combined_ocr = "\n".join(ocr_parts).strip()

    # 4. GPTë¡œ ì •êµí™”(íŠ¹ìˆ˜ê¸°í˜¸&ê´„í˜¸ ì‚½ì…)
    print("ğŸ”§ GPT ì •êµí™” ë‹¨ê³„ ì§„í–‰ ì¤‘...")
    refined_text = refine_text_with_gpt(image, combined_ocr)

    # 5. íŠ¹ìˆ˜ê¸°í˜¸ ìœ„ì¹˜ ê²€í†  ë° ë³´ì • => ì„±ê³µ
    print("ğŸ§  íŠ¹ìˆ˜ê¸°í˜¸ ìœ„ì¹˜ ê²€í†  ë° ë³´ì • ì¤‘...")
    symbol_corrected_text = verify_special_symbols(image, refined_text)

    # 6. ì´ë¯¸ì§€ ìš°ì¸¡ ì§€ë¬¸ ë²”ìœ„ í‘œì‹œ
    print("ğŸ—‚ï¸ GPT ì§€ë¬¸ ë²”ìœ„ í‘œì‹œ([A], [B] ë“±) ì‚½ì… ì¤‘...")
    final_result = insert_passage_brackets_with_gpt(image, symbol_corrected_text)

    return final_result.strip()

"""### **ë¬¸ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ**"""

def extract_question(question_path):
    import mimetypes

    # íŒŒì¼ í™•ì¥ìì— ë§ê²Œ MIME íƒ€ì… ì¶”ì •
    mime_type, _ = mimetypes.guess_type(question_path)
    if not mime_type:
        mime_type = "image/png"  # ê¸°ë³¸ê°’ fallback

    with open(question_path, "rb") as f:
        base64_img = base64.b64encode(f.read()).decode("utf-8")

    image_url = f"data:{mime_type};base64,{base64_img}"

    system_prompt = """
    ë„ˆëŠ” ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ ë¬¸ì œ ì´ë¯¸ì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì •í™•í•˜ê²Œ ë³µì›í•˜ëŠ” OCR ëª¨ë¸ì´ì•¼.
    ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ë³´ê³  **ë¬¸ì œì˜ í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¶”ì¶œ**í•´ì•¼ í•´.

    ì…ë ¥ ì´ë¯¸ì§€ì—ëŠ” ë‹¤ìŒì´ í¬í•¨ë  ìˆ˜ ìˆì–´:
    - ë¬¸í•™ ë¬¸ì œì˜ ì§ˆë¬¸ ë¬¸ì¥
    - â‘ ~â‘¤ ë³´ê¸° ì„ íƒì§€
    - ê²½ìš°ì— ë”°ë¼, ì§ˆë¬¸ ë¬¸ì¥ **ë’¤ì—** ì œì‹œë˜ëŠ” '<ë³´ê¸°>' ë¬¸ì¥

    ğŸ”¹ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ì¶œë ¥ ê·œì¹™:

    1. ì´ë¯¸ì§€ì— '<ë³´ê¸°>'ê°€ ì¡´ì¬í•˜ë©´, ì§ˆë¬¸ ë¬¸ì¥ **ë’¤ì—** <ë³´ê¸°> ì „ì²´ ë‚´ìš©ì„ ì •í™•íˆ í¬í•¨í•´ì•¼ í•´.
    2. <ë³´ê¸°>ê°€ ì—†ë‹¤ë©´, ì§ˆë¬¸ ë¬¸ì¥ ë‹¤ìŒì— ë°”ë¡œ ì„ íƒì§€ë¥¼ ì¶œë ¥í•´.
    3. ì„ íƒì§€ëŠ” í•­ìƒ â‘ ~â‘¤ ëª¨ë‘ ë¹ ì§ì—†ì´ ì¶œë ¥í•  ê²ƒ.
    4. ì¶œë ¥ í˜•ì‹ì€ ì•„ë˜ì™€ ê°™ì•„ì•¼ í•´:

    (ì§ˆë¬¸ ë¬¸ì¥)

    <ë³´ê¸°>
    (ë³´ê¸° ë‚´ìš©)

    â‘  ...
    â‘¡ ...
    â‘¢ ...
    â‘£ ...
    â‘¤ ...

    â—ì£¼ì˜:
    - <ë³´ê¸°>ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ê³ , **ì§ˆë¬¸ ë’¤ì— ìœ„ì¹˜**ì‹œì¼œì•¼ í•´.
    - ì¤„ë°”ê¿ˆ, ê¸°í˜¸, ë¬¸ì¥ë¶€í˜¸ ë“±ì€ ìµœëŒ€í•œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³µì›í•´.
    - ì ˆëŒ€ ì„¤ëª…, í•´ì„¤, ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë“± **ì¶”ê°€ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆ**.
    - ì˜¤ì§ ì´ë¯¸ì§€ ì† ë¬¸ì œ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•´.
    """

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt.strip()},
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": image_url
                        }
                    }
                ]
            }
        ],
        max_tokens=1500,
        temperature=0.1
    )

    return response.choices[0].message.content.strip()

"""# **<ë‹µë³€,í•´ì„¤ ëª¨ë¸>**"""

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.chat_models import ChatOpenAI
from langchain_core.runnables import RunnableLambda

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

# ê°œë… ì„¤ëª… ì²´ì¸
concept_prompt = ChatPromptTemplate.from_template("""
ë„ˆëŠ” í•œêµ­ì˜ ëŒ€í•™ìˆ˜í•™ëŠ¥ë ¥ì‹œí—˜ì˜ êµ­ì–´ ê³¼ëª© ì¤‘ ë¬¸í•™ì˜ ê°œë…ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ëŠ” íŠœí„°ì•¼. ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì„¤ëª…í•´ì¤˜.

ì§ˆë¬¸:
{question}
""")

concept_chain = concept_prompt | llm | StrOutputParser()

llm = ChatOpenAI(model="gpt-4o", temperature=0.3)

# <1. ê°œë… ì„¤ëª… chain>
concept_prompt = ChatPromptTemplate.from_template("""
ë„ˆëŠ” í•œêµ­ì˜ ëŒ€í•™ìˆ˜í•™ëŠ¥ë ¥ì‹œí—˜ì˜ êµ­ì–´ ê³¼ëª© ì¤‘ ë¬¸í•™ì˜ ê°œë…ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ëŠ” íŠœí„°ì•¼. ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì„¤ëª…í•´ì¤˜.

ì§ˆë¬¸:
{question}
""")
concept_chain = concept_prompt | llm | StrOutputParser()


# ë¬¸ì œ ê¸°ë°˜ QA ì²´ì¸ìš© í”„ë¡¬í”„íŠ¸
qa_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ í•œêµ­ ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ ì „ë¬¸ íŠœí„°ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ ê°ê´€ì‹ ë¬¸ì œì…ë‹ˆë‹¤. â‘ , â‘¡, â‘¢, â‘£, â‘¤ ì¤‘ í•˜ë‚˜ë¥¼ ê³ ë¥´ëŠ” ê°ê´€ì‹ ë¬¸ì œì´ë©°,
ì •ë‹µì€ ë°˜ë“œì‹œ ì§€ë¬¸ ë° <ë³´ê¸°>ì˜ ì •í™•í•œ ë¶„ì„ì— ê·¼ê±°í•´ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.

ğŸŸ¨ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ë¶„ì„ ê¸°ì¤€:

1. ì§€ë¬¸ ë¶„ì„ì„ ì¤‘ì‹¬ìœ¼ë¡œ íŒë‹¨í•˜ë©°, **ì„ íƒì§€ì˜ ë‚´ìš©ì´ ì§€ë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€** ì—„ë°€íˆ ê²€í† í•˜ì„¸ìš”.
2. ë¬¸ì œì— <ë³´ê¸°>ê°€ ìˆë‹¤ë©´, <ë³´ê¸°> ì† ì„¤ëª…(êµ¬ì¡°, ì‹œì , í‘œí˜„, ì¸ë¬¼ í•´ì„ ë“±)ì„ **ì§€ë¬¸ì— ì–´ë–»ê²Œ ì ìš©í–ˆëŠ”ì§€** êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
3. **ì„œìˆ  ë°©ì‹, ì¸ë¬¼ ì‹¬ë¦¬ í‘œí˜„, ë¬¸ì²´, ì‹œì  ë³€í™”, ì§€ì‹œ í‘œí˜„, ë³‘ë ¬ êµ¬ì¡°** ë“±ì€ ì„œìˆ ìƒÂ·í‘œí˜„ìƒ íŠ¹ì§• ë¬¸ì œì—ì„œ í•µì‹¬ ê·¼ê±°ì…ë‹ˆë‹¤.
4. ì°¸ê³  ìë£ŒëŠ” ë°˜ë“œì‹œ ë³´ì¡°ì  ìš©ë„ë¡œë§Œ í™œìš©í•©ë‹ˆë‹¤. ì ˆëŒ€ ì§€ë¬¸ì„ ë®ì–´ì“°ê±°ë‚˜ ëŒ€ì²´í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.

ğŸ“Œ ë‹µë³€ í˜•ì‹:

[ì •ë‹µ]
- (â‘ , â‘¡, â‘¢, â‘£, â‘¤ ì¤‘ í•˜ë‚˜)

[í•´ì„¤]
- ë¬¸ì œì—ì„œ ìš”êµ¬í•œ í•µì‹¬ ìš”ì†Œ(ì˜ˆ: í‘œí˜„ ë°©ì‹, êµ¬ì¡°, ì‹œì  ë“±)ì— ë”°ë¼ ì™œ ì •ë‹µì¸ì§€ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
- ì§€ë¬¸ ë° <ë³´ê¸°>ì˜ ë¬¸ì¥ì„ **ì§ì ‘ ì¸ìš©**í•˜ì—¬ ëª…í™•í•œ íŒë‹¨ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”.
- ë‚˜ë¨¸ì§€ ì˜¤ë‹µ ì„ íƒì§€ë“¤ì€ ê°ê° ì™œ í‹€ë ¸ëŠ”ì§€ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”.

ğŸ“– [ì§€ë¬¸]
{context}

ğŸ“š [ì°¸ê³  ìë£Œ] â€” í•„ìš” ì‹œë§Œ ì‚¬ìš© (retriever ì œê³µ):
{reference}

ğŸ™‹â€â™‚ï¸ [ë¬¸ì œ ë° <ë³´ê¸°>]
{question}
""")

def format_with_retrieved_docs(inputs):
    question = inputs["question"]
    context = inputs["context"]

    # ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (contextì™€ question ëª¨ë‘ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰)
    retrieved_docs = retriever_literature_answer.get_relevant_documents(f"{context}\n\n{question}")
    retrieved_context = "\n\n".join(doc.page_content for doc in retrieved_docs)

    return {
        "context": context,           # ì§€ë¬¸ ë° <ë³´ê¸°>
        "reference": retrieved_context,  # ë³´ì¡° ìë£Œ
        "question": question
    }

# <2. ë‹µë³€, í•´ì„¤ QA ì²´ì¸>: ë¬¸í•­ ì§€ë¬¸ ì¤‘ì‹¬ + retrieverì—ì„œ ê²€ìƒ‰í•œ ë‚´ìš© ì°¸ê³ 
rag_qa_chain = (
    RunnableLambda(format_with_retrieved_docs)
    | qa_prompt
    | llm
    | StrOutputParser()
)



# <ë¬¸ì œ vs ê°œë… ë¶„ë¥˜ í•¨ìˆ˜>
def is_problem_question(question: str) -> bool:
    classification_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ ì§ˆë¬¸ì´ ë¬¸í•™ ê°œë… ì§ˆë¬¸ì¸ì§€, ì§€ë¬¸ ê¸°ë°˜ ë¬¸ì œì¸ì§€ íŒë³„í•´ì¤˜. 'ê°œë…' ë˜ëŠ” 'ë¬¸ì œ' ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´.

ì§ˆë¬¸:
{question}
""")
    chain = classification_prompt | llm | StrOutputParser()
    result = chain.invoke({"question": question})
    return "ë¬¸ì œ" in result.strip()

# <ìµœì¢… ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜>
def tutor_response(question: str, passage: str = None) -> str:
    if is_problem_question(question):
        if not passage:
            print("â— ì˜¤ë¥˜: ë¬¸í•™ ë¬¸ì œ í’€ì´ì—ëŠ” ì§€ë¬¸(passage)ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return "[â— ì˜¤ë¥˜: ì§€ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤]"

        print("ğŸ“˜ [ë¬¸ì œì— ëŒ€í•œ ì •ë‹µ ë° í•´ì„¤]")
        response = rag_qa_chain.invoke({
            "context": passage,
            "question": question
        })
        return response
    else:
        print("ğŸ“˜ [ë¬¸í•™ ê°œë… ì„¤ëª…]")
        response = concept_chain.invoke({"question": question})
        return response

"""# **<ìœ ì‚¬ ê¸°ì¶œë¬¸ì œ ì¶”ì²œ ëª¨ë¸>**"""

# <ì‚¬ìš©ì ì§ˆë¬¸ì˜ GPT íƒœê¹…>
def get_tags_from_gpt(query):
    prompt = f"""
            ë‹¤ìŒ ë¬¸í•™ ì§€ë¬¸ê³¼ ë¬¸ì œë¥¼ ì½ê³  ì•„ë˜ í•­ëª©ì„ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

            ğŸ“š ì…ë ¥ ì •ë³´
            (ì§€ë¬¸&ë¬¸ì œ)
            {query}

            ğŸ§© ë³µí•©/ë‹¨ì¼ íŒë‹¨ ê¸°ì¤€:
            - ì§€ë¬¸ì´ 2ê°œ ì´ìƒì´ë©´ ë°˜ë“œì‹œ "ë³µí•©"ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
            - ë¬¸ì œì—ì„œ 'ê³µí†µì ', 'ë¹„êµ', 'ë‹¤ìŒ ê¸€ë“¤', '(ê°€)ì™€ (ë‚˜)'ë¼ëŠ” í‘œí˜„ì´ ë“±ì¥í•˜ë©´ ë°˜ë“œì‹œ "ë³µí•©"ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
            - ìœ„ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¼ë„ ì¶©ì¡±í•˜ë©´ ë°˜ë“œì‹œ "ë³µí•©"ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
            - ì§€ë¬¸ì´ 1ê°œì´ê±°ë‚˜ ì§€ë¬¸ ì—¬ëŸ¬ê°œ ì¤‘ì—ì„œ ë¬¸ì œì—ì„œ í•œ íŠ¹ì • ì§€ë¬¸ë§Œ ë¬»ëŠ” ê²½ìš°ì—ë§Œ "ë‹¨ì¼"ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
            âš ï¸ ë³µí•©/ë‹¨ì¼ ë¶„ë¥˜ëŠ” ì ˆëŒ€ í‹€ë¦¬ë©´ ì•ˆ ë©ë‹ˆë‹¤. ë°˜ë“œì‹œ ì£¼ì˜í•˜ì„¸ìš”.

            ğŸ§  ë¬¸ì œ ìœ í˜• ë¶„ë¥˜ ê¸°ì¤€ (ì§€ë¬¸ ì¥ë¥´ë³„ë¡œ ì•„ë˜ ì¤‘ í•˜ë‚˜ ì„ íƒ):
            - í˜„ëŒ€ì‹œ:
              - ë‚´ìš© ì´í•´: ì‹œì  ìƒí™©, ì£¼ì œ, ì •ì„œ ë° íƒœë„ ë“± íŒŒì•…
              - ì •ì„œ ë° íƒœë„ íŒŒì•…: í™”ìì˜ ì‹¬ë¦¬ì™€ íƒœë„ íë¦„ ì´í•´
              - í‘œí˜„ ë°©ì‹ ë¶„ì„: ë¹„ìœ , ìƒì§•, ë°˜ë³µ, ì„¤ì˜ ë“±
              - ì‹œì–´ í•´ì„: ê°œë³„ ì‹œì–´ ë˜ëŠ” í‘œí˜„ì˜ ìƒì§•/ì¤‘ì˜ì  ì˜ë¯¸ í•´ì„
              - ìƒì§•/ë¹„ìœ  í•´ì„: ì¤‘ì‹¬ ì´ë¯¸ì§€ë‚˜ ìƒì§• êµ¬ì¡° í•´ì„

            - ê³ ì „ ì‹œê°€:
              - ë‚´ìš© ì´í•´: ì „ì²´ ì˜ë¯¸, ì‘í’ˆ íë¦„, ì •ì„œ ì´í•´
              - ì •ì„œ íŒŒì•…: ì„ì— ëŒ€í•œ ë§ˆìŒ, ìì—°/í˜„ì‹¤ ì¸ì‹
              - í‘œí˜„ ê¸°ë²• ë¶„ì„: ê³ ì „ì  ìˆ˜ì‚¬ ê¸°ë²• ë¶„ì„ (ì˜íƒ„, ëŒ€ì¡°, ê³¼ì¥ ë“±)
              - ì„-í™”ì ê´€ê³„ ì´í•´: êµìˆ /ì„œì • ì‹œê°€ì—ì„œì˜ ê´€ê³„ ë§¥ë½
              - ë³€ì‹ /í™˜ìƒ í‘œí˜„ í•´ì„: ì‹ í™”/í™˜ìƒì  ìš”ì†Œ í•´ì„

            - í˜„ëŒ€ ì†Œì„¤:
              - ì‚¬ê±´ íë¦„ íŒŒì•…: ì¤„ê±°ë¦¬ ë° ì£¼ìš” ì‚¬ê±´ íë¦„ ì´í•´
              - ì¸ë¬¼ ì‹¬ë¦¬ ì´í•´: ì¸ë¬¼ì˜ ì„±ê²©, ë‚´ì  ì‹¬ë¦¬, ê´€ê³„ í•´ì„
              - ì‹œì  ë° ì„œìˆ  ë°©ì‹ ë¶„ì„: ì„œìˆ ì, ì‹œì , ë¬˜ì‚¬ ë°©ì‹ ë¶„ì„
              - ì£¼ì œ/ì‘ê°€ ì˜ë„ íŒŒì•…: ì¤‘ì‹¬ ì£¼ì œ, ì£¼ì œì˜ì‹ ë¶„ì„
              - ê³µê°„/ë°°ê²½ ì˜ë¯¸ ë¶„ì„: ë°°ê²½ì´ ê°€ì§€ëŠ” ìƒì§•ì  ì˜ë¯¸ í•´ì„

            - ê³ ì „ ì†Œì„¤:
              - ë‚´ìš© ì´í•´: ì¤„ê±°ë¦¬, ì‚¬ê±´ êµ¬ì¡° íŒŒì•…
              - ì¸ë¬¼ ì‹¬ë¦¬ ë° ìš´ëª… íŒŒì•…: ì£¼ìš” ì¸ë¬¼ì˜ ì„±ê²©ê³¼ ìš´ëª…
              - ìƒì§• ì¥ì¹˜ í•´ì„: ê¿ˆ, ì „ê¸°, ìì—° ìš”ì†Œ ë“± ìƒì§• êµ¬ì¡° í•´ì„
              - ê¶Œì„ ì§•ì•…ì  ê´€ì  ë¶„ì„: ì¸ê³¼ì  ì„¸ê³„ê´€, ë„ë•ì  êµí›ˆ í•´ì„
              - ì„œì‚¬ êµ¬ì¡° ë¶„ì„: ë„ì…-ì „ê°œ-ìœ„ê¸°-ì ˆì •-ê²°ë§ì˜ êµ¬ì¡°

            - ê·¹/ìˆ˜í•„:
              - ë‚´ìš© ì´í•´: ìƒí™©, ëŒ€ì‚¬, ì‚¬ê±´ì˜ íë¦„ ì´í•´
              - í‘œí˜„ íŠ¹ì„± ë¶„ì„: ëŒ€ì‚¬, í•´ì„¤, ì¥ë©´ êµ¬ì„±ì˜ íŠ¹ì§•
              - ì„œìˆ ìì˜ ê°œì… íŒŒì•…: ìˆ˜í•„/ê·¹ ì¤‘ ì„œìˆ ìì˜ ìœ„ì¹˜ ë° ì—­í• 
              - ì£¼ì œ ë° êµí›ˆ ë„ì¶œ: ì¤‘ì‹¬ ì£¼ì œ ë° ì‚¶ì— ì£¼ëŠ” êµí›ˆ í•´ì„

            âœ’ï¸ ì§€ë¬¸ ì œëª© ë° ì‘ê°€ ì¶”ì¶œ ê¸°ì¤€:
            - ì¼ë°˜ì ìœ¼ë¡œ ê° ì§€ë¬¸ ëì— ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í‘œê¸°ë¨:
              `- ê¹€ìˆ˜ì˜ , ï½¢ê·¸ ë°©ì„ ìƒê°í•˜ë©°ï½£ -`
              â†’ ì‘ê°€: "ê¹€ìˆ˜ì˜", ì œëª©: "ê·¸ ë°©ì„ ìƒê°í•˜ë©°"
            - ë³µí•© ì§€ë¬¸ì¼ ê²½ìš°:
              - "ì§€ë¬¸ ì œëª©": ["ì œëª©1", "ì œëª©2", ...]
              - "ì§€ë¬¸ ì‘ê°€": ["ì‘ê°€1", "ì‘ê°€2", ...]
              - ì‘ì ë¯¸ìƒì¼ ê²½ìš° "ì‘ì ë¯¸ìƒ"ìœ¼ë¡œ í‘œê¸°
              âš ï¸ ì œëª© ë˜ëŠ” ì‘ê°€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°:
              - ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•˜ì„¸ìš”.
              - ì§€ë¬¸ ì œëª©: "ì§€ë¬¸ ì œëª© ì—†ìŒ"
              - ì§€ë¬¸ ì‘ê°€: "ì‘ì ë¯¸ìƒ"
              - ë³µí•© ì§€ë¬¸ì¼ ê²½ìš°: ["ì§€ë¬¸ ì œëª© ì—†ìŒ", "ì§€ë¬¸ ì œëª© ì—†ìŒ"], ["ì‘ì ë¯¸ìƒ", "ì‘ì ë¯¸ìƒ"]

            ğŸ“Œ ì¶œë ¥ í˜•ì‹ (ëª¨ë‘ í¬í•¨):
            - type: ë°˜ë“œì‹œ "ë¬¸í•™"
            - ì§€ë¬¸ ì œëª©: ë¬¸ìì—´ ë˜ëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
            - ì§€ë¬¸ ì¥ë¥´: ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ (ê°ˆë˜ ë³µí•©ì´ë©´ ë¦¬ìŠ¤íŠ¸)
            - ì§€ë¬¸ ì‘ê°€: ë¬¸ìì—´ ë˜ëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
            - ë³µí•©/ë‹¨ì¼: "ë³µí•©" ë˜ëŠ” "ë‹¨ì¼"
            - ë¬¸ì œ ìœ í˜•: ìœ„ ê¸°ì¤€ ì¤‘ ì ì ˆí•œ ê²ƒ í•˜ë‚˜

            ğŸ“Œ ì¶œë ¥ ì˜ˆì‹œ:

            [ì§€ë¬¸ 1ê°œ ì˜ˆì‹œ]
            {{
              "type": "ë¬¸í•™",
              "ì§€ë¬¸ ì œëª©": "ìˆ™í–¥ì „",
              "ì§€ë¬¸ ì¥ë¥´": "ê³ ì „ ì†Œì„¤",
              "ì§€ë¬¸ ì‘ê°€": "ì‘ì ë¯¸ìƒ",
              "ë³µí•©/ë‹¨ì¼": "ë‹¨ì¼",
              "ë¬¸ì œ ìœ í˜•": "ì¸ë¬¼ ì‹¬ë¦¬ ë° ìš´ëª… íŒŒì•…"
            }}

            [ì§€ë¬¸ ì—¬ëŸ¬ê°œ ì˜ˆì‹œ]
            {{
              "type": "ë¬¸í•™",
              "ì§€ë¬¸ ì œëª©": ["ë³„ì‚¬ë¯¸ì¸ê³¡", "ì œëª© ì—†ìŒ", "ë°±ìì¦ì •ë¶€ì¸ë°•ì”¨ë¬˜ì§€ëª…"],
              "ì§€ë¬¸ ì¥ë¥´": ["ê³ ì „ ì‹œê°€", "ê³ ì „ ì‹œê°€", "ê³ ì „ ì‚°ë¬¸"],
              "ì§€ë¬¸ ì‘ê°€": ["ê¹€ì¶˜íƒ", "ì´ì •ë³´", "ë°•ì§€ì›"],
              "ë³µí•©/ë‹¨ì¼": "ë³µí•©",
              "ë¬¸ì œ ìœ í˜•": "ì •ì„œ íŒŒì•…"
            }}
        """

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ ì „ë¬¸ íƒœê¹… ë„ìš°ë¯¸ì…ë‹ˆë‹¤. âš ï¸ ë°˜ë“œì‹œ ì½”ë“œ ë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
        max_tokens=1000
    )

    content = response.choices[0].message.content.strip()
    # GPTê°€ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ê°ìŒ€ ê²½ìš° ì œê±°
    content = re.sub(r"```json\s*([\s\S]+?)\s*```", r"\1", content)
    content = re.sub(r"```[\s\S]+?```", "", content).strip()

    try:
        return json.loads(content)
    except json.JSONDecodeError:
        print("âŒ GPT ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨:\n", content)
        return None


import json
import re
import openai

# ğŸ”¸ GPT í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = openai.OpenAI()

# ğŸ”¸ ë¬¸ì œ ì½”ë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_question_code(source_str):
    if not source_str:
        return None
    source_str = source_str.replace("á„€á…®á†¨á„‹á…¥", "êµ­ì–´")
    match = re.search(r'(\d{4}-(?:\d{2}|ìˆ˜ëŠ¥)-êµ­ì–´_\d+)$', source_str)
    if match:
        return match.group(1)
    all_matches = re.findall(r'(\d{4}-(?:\d{2}|ìˆ˜ëŠ¥)-êµ­ì–´_\d+)', source_str)
    if all_matches:
        return all_matches[-1]
    return source_str


# ğŸ”¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì— íƒœê·¸ ë³‘í•©
def merge_tags_to_docs(docs, tag_dict):
    for doc in docs:
        source = doc.metadata.get("source")
        code = extract_question_code(source)
        if code and code in tag_dict:
            doc.metadata["question_tags"] = tag_dict[code]
    return docs


# ğŸ”¹ íƒœê·¸ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (ë¶€ë¶„ ì ìˆ˜ í¬í•¨)
def tag_similarity_score(user_tags, doc_tags):
    score = 0

    # ë¬¸ì œ ìœ í˜• (4ì , ì™„ì „ ì¼ì¹˜ ì‹œ)
    if user_tags.get("ë¬¸ì œ ìœ í˜•") == doc_tags.get("ë¬¸ì œ ìœ í˜•"):
        score += 4

    # ë³µí•©/ë‹¨ì¼ (2ì )
    if user_tags.get("ë³µí•©/ë‹¨ì¼") == doc_tags.get("ë³µí•©/ë‹¨ì¼"):
        score += 2

    # ì§€ë¬¸ ì¥ë¥´ (ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬)
    user_genre = user_tags.get("ì§€ë¬¸ ì¥ë¥´")
    doc_genre = doc_tags.get("ì§€ë¬¸ ì¥ë¥´")

    if user_genre == doc_genre:
        score += 5
    else:
        # ë¹„ìŠ·í•œ ì¥ë¥´ ë¶€ë¶„ ì ìˆ˜
        genre_similarities = [
            ("í˜„ëŒ€ì‹œ", "ê³ ì „ ì‹œê°€"),
            ("ê³ ì „ ì‹œê°€", "í˜„ëŒ€ì‹œ"),
            ("í˜„ëŒ€ ì†Œì„¤", "ê³ ì „ ì†Œì„¤"),
            ("ê³ ì „ ì†Œì„¤", "í˜„ëŒ€ ì†Œì„¤"),
        ]
        if (user_genre, doc_genre) in genre_similarities or (doc_genre, user_genre) in genre_similarities:
            score += 2

    # ì§€ë¬¸ ì œëª© (1ì )
    user_title = user_tags.get("ì§€ë¬¸ ì œëª©")
    doc_title = doc_tags.get("ì§€ë¬¸ ì œëª©")

    if isinstance(user_title, list) and isinstance(doc_title, list):
        if set(user_title) & set(doc_title):
            score += 1
    elif isinstance(user_title, list):
        if doc_title in user_title:
            score += 1
    elif isinstance(doc_title, list):
        if user_title in doc_title:
            score += 1
    else:
        if user_title == doc_title:
            score += 1

    # ì§€ë¬¸ ì‘ê°€ (1ì )
    user_name = user_tags.get("ì§€ë¬¸ ì‘ê°€")
    doc_name = doc_tags.get("ì§€ë¬¸ ì‘ê°€")

    if isinstance(user_name, list) and isinstance(doc_name, list):
        if set(user_name) & set(doc_name):
            score += 1
    elif isinstance(user_name, list):
        if doc_name in user_name:
            score += 1
    elif isinstance(doc_name, list):
        if user_name in doc_name:
            score += 1
    else:
        if user_name == doc_name:
            score += 1

    return score

import json
import os
import re
import unicodedata
from PIL import Image as PILImage
from IPython.display import Image as DisplayImage, display
import streamlit as st

# ì¶œì²˜ ì½”ë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_question_code(source_str):
    if not source_str:
        return None
    source_str = unicodedata.normalize('NFC', source_str)
    source_str = source_str.replace("á„€á…®á†¨á„‹á…¥", "êµ­ì–´").replace("á„‰á…®á„‚á…³á†¼", "ìˆ˜ëŠ¥")

    # ì¶œì²˜ ì½”ë“œ ì •ê·œì‹ ì¶”ì¶œ (ëì— ìˆëŠ” ì½”ë“œ ìš°ì„ )
    match = re.search(r'(\d{4}-(?:\d{2}|ìˆ˜ëŠ¥)-êµ­ì–´_\d+)$', source_str)
    if match:
        return match.group(1)
    # ì „ì²´ ì¤‘ ë§ˆì§€ë§‰ ì½”ë“œ ì¶”ì¶œ
    all_matches = re.findall(r'(\d{4}-(?:\d{2}|ìˆ˜ëŠ¥)-êµ­ì–´_\d+)', source_str)
    return all_matches[-1] if all_matches else None

# ì´ë¯¸ì§€ ì €ì¥ ë£¨íŠ¸ í´ë”
IMAGE_ROOT = "/content/drive/MyDrive/Colab Notebooks/TAVE í”„ë¡œì íŠ¸_STUBO/ìˆ˜ëŠ¥ êµ­ì–´ AI íŠœí„°ë§ ì‹œìŠ¤í…œ/ë¬¸í•™/data/output_images"

# ë¬¸ì œ ì´ë¯¸ì§€ ê²½ë¡œ ë°˜í™˜
def get_problem_image_path(question_code):
    if not question_code:
        return None
    return os.path.join(IMAGE_ROOT, f"{question_code}.png")

# ì§€ë¬¸ ì´ë¯¸ì§€ ê²½ë¡œ ë°˜í™˜
def get_passage_image_path(passage_code):
    if not passage_code:
        return None
    passage_code = unicodedata.normalize("NFC", passage_code)
    match = re.match(r"(\d{4}-(?:\d{2}|ìˆ˜ëŠ¥)-êµ­ì–´)(_p\d+)", passage_code)
    if not match:
        print(f"âŒ ì§€ë¬¸ ì½”ë“œ íŒŒì‹± ì‹¤íŒ¨: {passage_code}")
        return None
    base, p_part = match.groups()
    return os.path.join(IMAGE_ROOT, f"{base}{p_part}.png")

# ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì— íƒœê·¸ ë³‘í•©
def merge_tags_to_docs(docs, tag_dict):
    for doc in docs:
        source = doc.metadata.get("ì¶œì²˜") or doc.metadata.get("source")
        code = extract_question_code(source)
        if not code:
            print(f"âŒ [ë³‘í•© ì‹¤íŒ¨] ì¶œì²˜ ì½”ë“œ ì¶”ì¶œ ì‹¤íŒ¨: {source}")
            continue
        if code not in tag_dict:
            print(f"âŒ [ë³‘í•© ì‹¤íŒ¨] íƒœê·¸ ì—†ìŒ: {code}")
            continue
        doc.metadata["question_tags"] = tag_dict[code]
        doc.metadata["ì¶œì²˜"] = code
    return docs

def tag_similarity_score(user_tags, doc_tags):
    """
    ì‚¬ìš©ì íƒœê·¸(user_tags)ì™€ ë¬¸ì„œ íƒœê·¸(doc_tags)ë¥¼ ë¹„êµí•˜ì—¬ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    ì ìˆ˜ ë°°ì :
    - ë¬¸ì œ ìœ í˜•: 2ì  (ì™„ì „ ì¼ì¹˜ ì‹œ)
    - ë³µí•©/ë‹¨ì¼: 2ì  (ì¼ì¹˜ ì‹œ)
    - ì§€ë¬¸ ì¥ë¥´: 3ì  (ì™„ì „ ì¼ì¹˜), 2ì  (ìœ ì‚¬ ì¥ë¥´)
    - ì§€ë¬¸ ì œëª©: 1ì  (í•˜ë‚˜ë¼ë„ ì¼ì¹˜ ì‹œ)
    - ì§€ë¬¸ ì‘ê°€: 1ì  (í•˜ë‚˜ë¼ë„ ì¼ì¹˜ ì‹œ)
    """
    score = 0

    # 1) ë¬¸ì œ ìœ í˜• (4ì )
    if user_tags.get("ë¬¸ì œ ìœ í˜•") == doc_tags.get("ë¬¸ì œ ìœ í˜•"):
        score += 2

    # 2) ë³µí•©/ë‹¨ì¼ (2ì )
    if user_tags.get("ë³µí•©/ë‹¨ì¼") == doc_tags.get("ë³µí•©/ë‹¨ì¼"):
        score += 2

    # 3) ì§€ë¬¸ ì¥ë¥´ (5ì  ì™„ì „ ì¼ì¹˜, 2ì  ìœ ì‚¬ ì¥ë¥´)
    user_genre = user_tags.get("ì§€ë¬¸ ì¥ë¥´")
    doc_genre = doc_tags.get("ì§€ë¬¸ ì¥ë¥´")

    if user_genre == doc_genre:
        score += 3
    else:
        # ìœ ì‚¬ ì¥ë¥´ ìŒ (ë¶€ë¶„ ì ìˆ˜)
        genre_similarities = [
            ("í˜„ëŒ€ì‹œ", "ê³ ì „ ì‹œê°€"),
            ("ê³ ì „ ì‹œê°€", "í˜„ëŒ€ì‹œ"),
            ("í˜„ëŒ€ ì†Œì„¤", "ê³ ì „ ì†Œì„¤"),
            ("ê³ ì „ ì†Œì„¤", "í˜„ëŒ€ ì†Œì„¤"),
        ]
        if (user_genre, doc_genre) in genre_similarities or (doc_genre, user_genre) in genre_similarities:
            score += 2

    # 4) ì§€ë¬¸ ì œëª© (1ì , í•˜ë‚˜ë¼ë„ ê²¹ì¹˜ë©´ ì ìˆ˜ ë¶€ì—¬)
    user_title = user_tags.get("ì§€ë¬¸ ì œëª©")
    doc_title = doc_tags.get("ì§€ë¬¸ ì œëª©")

    if isinstance(user_title, list) and isinstance(doc_title, list):
        if set(user_title) & set(doc_title):  # êµì§‘í•©ì´ ìˆìœ¼ë©´
            score += 1
    elif isinstance(user_title, list):
        if doc_title in user_title:
            score += 1
    elif isinstance(doc_title, list):
        if user_title in doc_title:
            score += 1
    else:
        if user_title == doc_title:
            score += 1

    # 5) ì§€ë¬¸ ì‘ê°€ (1ì , í•˜ë‚˜ë¼ë„ ê²¹ì¹˜ë©´ ì ìˆ˜ ë¶€ì—¬)
    user_name = user_tags.get("ì§€ë¬¸ ì‘ê°€")
    doc_name = doc_tags.get("ì§€ë¬¸ ì‘ê°€")

    if isinstance(user_name, list) and isinstance(doc_name, list):
        if set(user_name) & set(doc_name):
            score += 1
    elif isinstance(user_name, list):
        if doc_name in user_name:
            score += 1
    elif isinstance(doc_name, list):
        if user_name in doc_name:
            score += 1
    else:
        if user_name == doc_name:
            score += 1

    return score

def get_similar_problems_with_images(user_question, retriever, tag_dict, top_k=2):
    user_tags = get_tags_from_gpt(user_question)
    if user_tags is None:
        return []

    results = retriever.similarity_search_with_score(user_question, k=20)

    docs = []
    for doc, score in results:
        doc.metadata["score"] = score
        docs.append(doc)

    docs = merge_tags_to_docs(docs, tag_dict)

    docs_with_score = []
    for doc in docs:
        doc_tags = doc.metadata.get("question_tags")
        if not doc_tags:
            continue
        tag_sim = tag_similarity_score(user_tags, doc_tags)
        embedding_sim = doc.metadata.get("score", 0)
        final_score = round(tag_sim * 0.7 + embedding_sim * 0.3, 4)
        docs_with_score.append((doc, doc_tags, tag_sim, embedding_sim, final_score))

    docs_sorted = sorted(docs_with_score, key=lambda x: x[4], reverse=True)

    similar_problems = []
    for i, (doc, doc_tags, tag_sim, emb_sim, final_score) in enumerate(docs_sorted[:top_k]):
        question_code = extract_question_code(doc.metadata.get("ì¶œì²˜"))
        passage_code = doc_tags.get("ì§€ë¬¸")
        problem_img = get_problem_image_path(question_code)
        passage_img = get_passage_image_path(passage_code)

        similar_problems.append({
            "index": i + 1,
            "question_code": question_code,
            "final_score": final_score,
            "problem_img": problem_img,
            "passage_img": passage_img
        })

    return similar_problems


# íƒœê·¸ ë¡œë“œ ë° ë”•ì…”ë„ˆë¦¬ ìƒì„±
with open('/content/drive/MyDrive/Colab Notebooks/TAVE í”„ë¡œì íŠ¸_STUBO/ìˆ˜ëŠ¥ êµ­ì–´ AI íŠœí„°ë§ ì‹œìŠ¤í…œ/ë¬¸í•™/data/literature_tagged.json', 'r', encoding='utf-8') as f:
    tag_list = json.load(f)

tag_dict_literature = {}
for item in tag_list:
    code = extract_question_code(item.get("ì¶œì²˜"))
    if code:
        tag_dict_literature[code] = {
            "ì¶œì²˜": item.get("ì¶œì²˜"),
            "ë¬¸ì œ ìœ í˜•": item.get("ë¬¸ì œ ìœ í˜•"),
            "ë³µí•©/ë‹¨ì¼": item.get("ë³µí•©/ë‹¨ì¼"),
            "ì§€ë¬¸ ì œëª©": item.get("ì§€ë¬¸ ì œëª©"),
            "ì§€ë¬¸ ì¥ë¥´": item.get("ì§€ë¬¸ ì¥ë¥´"),
            "ì§€ë¬¸ ì‘ê°€": item.get("ì§€ë¬¸ ì‘ê°€"),
            "ì§€ë¬¸": item.get("ì§€ë¬¸"),
        }

"""# **<í†µí•© íŒŒì´í”„ë¼ì¸>**
1. ë¬¸ì œ ì´ë¯¸ì§€ ì…ë ¥ â†’ OCRë¡œ ë¬¸ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ

2. ì§€ë¬¸ ì´ë¯¸ì§€ ì…ë ¥ â†’ OCR í›„ êµ¬ì¡° ë³´ì •ëœ ì§€ë¬¸ í…ìŠ¤íŠ¸ ìƒì„±

3. ì •ë‹µ + í•´ì„¤ ìƒì„± (RAG ê¸°ë°˜)

4. ìœ ì‚¬ ê¸°ì¶œ ë¬¸ì œ ì¶”ì²œ (ì„ë² ë”© + íƒœê·¸ ê¸°ë°˜)

"""

def pipeline_literature(
    question_image_path: str,
    passage_image_path: str,
    retriever_answer,
    retriever_recommend,
    tag_dict,
    show_images: bool = False,
    recommend_top_k: int = 2
):
    """
    ğŸ” ë¬¸í•™ ë¬¸ì œ íŒŒì´í”„ë¼ì¸: OCR â†’ ë‹µë³€/í•´ì„¤ â†’ ìœ ì‚¬ë¬¸ì œ ì¶”ì²œ

    Parameters:
    - question_image_path: ë¬¸ì œ ì´ë¯¸ì§€ ê²½ë¡œ
    - passage_image_path: ì§€ë¬¸ ì´ë¯¸ì§€ ê²½ë¡œ
    - retriever_answer: ë‹µë³€ í•´ì„¤ìš© ë²¡í„°ìŠ¤í† ì–´ retriever
    - retriever_recommend: ìœ ì‚¬ë¬¸ì œ ì¶”ì²œìš© retriever
    - tag_dict: ê¸°ì¶œ íƒœê·¸ ë”•ì…”ë„ˆë¦¬
    - show_images: Streamlitìš© ì´ë¯¸ì§€ ì¶œë ¥ ì—¬ë¶€
    - recommend_top_k: ì¶”ì²œí•  ìœ ì‚¬ë¬¸ì œ ìˆ˜
    """

    print("ğŸ” [1ë‹¨ê³„] ë¬¸ì œ ì´ë¯¸ì§€ OCR ì¶”ì¶œ ì¤‘...")
    question_text = extract_question(question_image_path)

    print("\nğŸ” [2ë‹¨ê³„] ì§€ë¬¸ ì´ë¯¸ì§€ OCR ë° ì •êµí™” ì¤‘...")
    passage_text = run_split_pipeline(passage_image_path)

    print("\nğŸ§  [3ë‹¨ê³„] ë¬¸ì œ í’€ì´ ë° í•´ì„¤ ìƒì„± ì¤‘...")
    answer_explanation_text = tutor_response(
        question=question_text,
        passage=passage_text
    )

    print("\nğŸ“‚ [4ë‹¨ê³„] ìœ ì‚¬ ë¬¸ì œ ì¶”ì²œ ì‹¤í–‰ ì¤‘...")
    similar_problem_data = get_similar_problems_with_images(
        user_question=question_text,
        retriever=retriever_literature_recommend,
        tag_dict=tag_dict_literature,
        top_k=recommend_top_k
    )

    return {
        "question": question_text,
        "passage": passage_text,
        "response": answer_explanation_text,
        "similar_problems": similar_problem_data
    }